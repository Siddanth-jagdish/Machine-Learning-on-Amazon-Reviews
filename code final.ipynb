{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Product Review #\n",
    "# Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages \n",
    "# These modules is used to data set loding and reading. It consists of (.Json, .csv) files. \n",
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import json\n",
    "import csv\n",
    "print(\"************************Done*********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages \n",
    "# These modules are used to preprocess data, manipulate data, marge and separate data.import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import re\n",
    "print(\"************************Done*********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages \n",
    "# This modules are used to cluster the data, extracting features, Converting word to vector, Classification and perfomance evaluation.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "print(\"*********************Done*************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('reviews_Office_Products.json',lines='True' )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpfulness = df[['helpful','asin']]\n",
    "helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the helpful data frame to  helpful, total. It means Helpfulnes of the product eg: 2/3\n",
    "df2 = pd.DataFrame(helpfulness)\n",
    "df2[['helpful','total']] = pd.DataFrame(df2.helpful.values.tolist(), index=df2.index)\n",
    "dfhelpful = df2[['asin', 'helpful','total']]\n",
    "dfhelpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join total, helpful data frames to the main data frame. \n",
    "dfNew = df.drop('helpful',1)\n",
    "frames = [dfNew, dfhelpful]\n",
    "result = dfNew.join(dfhelpful[['helpful','total']])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create .csv file to store all modified data frame reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('reviews.csv', sep=',', header=False,index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load .csv data to sqlite for Fetch the score and recommendation summary using SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csvrd(object):\n",
    "    def csvFile(self):\n",
    "        self.readFile('reviews.csv')\n",
    "    def readFile(self, filename):\n",
    "        conn = sqlite3.connect('amazonReviews.db')\n",
    "        cur = conn.cursor() \n",
    "        cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS amazonReviews(asin INT,overall INT,reviewText varchar,reviewTime INTEGER, reviewerID varchar,reviewerName varchar,summary varchar,unixReviewTime INTEGER,helpful INT,total INT)\"\"\")\n",
    "        filename.encode('utf-8')\n",
    "        print (\"Amazon Reviews table executed in DB \")\n",
    "        with open(filename) as f:\n",
    "            reader = csv.reader(f)\n",
    "            for field in reader:\n",
    "                cur.execute(\"INSERT INTO amazonReviews VALUES (?,?,?,?,?,?,?,?,?,?);\", field)\n",
    "\n",
    "        print (\"**************CSV Loaded into SQLite******************\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "c = csvrd().csvFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data from SQLite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('amazonReviews.db')\n",
    "pd.read_sql_query(\"SELECT * FROM amazonReviews LIMIT 20\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring the reviews having overall score equal to 3. \n",
    "#If the overall score is above 3, then the label for it will be set to Positive\n",
    "#else it will be set to Negative\n",
    "reviews = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "  overall, \n",
    "  summary, \n",
    "  helpful, \n",
    "  total\n",
    "FROM amazonReviews \n",
    "WHERE overall != 3\"\"\", con)\n",
    "#display data\n",
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sentiment column depicts the numeric score of being positive or negative\n",
    "##usefulScore column depicts the boolean value of total number of votes\n",
    "reviews[\"sentiment\"] = reviews[\"overall\"].apply(lambda score: \"positive\" if score > 3 else \"negative\")\n",
    "reviews[\"usefulScore\"] = (reviews[\"helpful\"]/reviews[\"total\"]).apply(lambda n: \"useful\" if n > 0.8 else \"useless\")\n",
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews over all score is equal to 5\n",
    "reviews[reviews.overall == 5].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Rows having overall score equal to 1\n",
    "reviews[reviews.overall == 1].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Feature extraction from collected reviews text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new dimension to each word and give the word counts\n",
    "regEx = re.compile('[^a-z]+')\n",
    "def cleanReviews(reviewText):\n",
    "    reviewText = reviewText.lower()\n",
    "    reviewText = regEx.sub(' ', reviewText).strip()\n",
    "    print(reviewText)\n",
    "    return reviewText\n",
    "reviews[\"summaryClean\"] = reviews[\"summary\"].apply(cleanReviews)\n",
    "\n",
    "train, test = train_test_split(reviews.sample(n=150000, random_state=1), test_size=0.2)\n",
    "print(\"%d items in training data, %d in test data\" % (len(train), len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using TfidfTransformer().fit_transofrm \n",
    "##to fit the train and test data\n",
    "countVector = CountVectorizer(min_df = 1, ngram_range = (1, 4))\n",
    "X_train_counts = countVector.fit_transform(train[\"summaryClean\"])\n",
    "\n",
    "\n",
    "\n",
    "#Applying tfidf to term frequency\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_new_counts = countVector.transform(test[\"summaryClean\"])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "y_train = train[\"sentiment\"]\n",
    "y_test = test[\"sentiment\"]\n",
    "\n",
    "prediction = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = df.groupby(\"overall\")[\"summary\"].apply(list)\n",
    "cluster = pd.DataFrame(cluster)\n",
    "cluster.to_csv(\"cluster.csv\")\n",
    "#writing cluster data into .csv file. \n",
    "cluster1 = pd.read_csv(\"cluster.csv\")\n",
    "cluster1[\"summaryClean\"] = cluster1[\"summary\"].apply(cleanReviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop words cleaning \n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "#mpl.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(reviews[\"summaryClean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(reviews[reviews.overall == 1][\"summaryClean\"], title = \"Low scored words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(reviews[reviews.overall == 5][\"summaryClean\"], title = \"High scored words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(reviews[reviews.overall == 2][\"summaryClean\"], title = \"Average scored words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes with Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Multinomial'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Bernoulli'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg_result = logreg.fit(X_train_tfidf, y_train)\n",
    "prediction['Logistic'] = logreg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train_tfidf, y_train)\n",
    "prediction['SVM'] = svclassifier.predict(X_test_tfidf)\n",
    "#classifier = KNeighborsClassifier(n_neighbors=5,n_jobs=5)\n",
    "#classifier.fit(X_train_tfidf, y_train)\n",
    "# Import the model we are using\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "#rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "#rf.fit(X_train_tfidf, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "prediction['KNN'] = classifier.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "prediction['DecisionTree'] = clf.predict(X_test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['SVM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['KNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['Logistic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Multinomial accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['Multinomial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Bernoulli accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['Bernoulli'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['DecisionTree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatt(x):\n",
    "    if x == 'negative':\n",
    "        return 0\n",
    "    return 1\n",
    "vfunc = np.vectorize(formatt)\n",
    "\n",
    "cmp = 0\n",
    "colors = ['b', 'g', 'y', 'm', 'k','r']\n",
    "plt.figure(figsize=(20,10))\n",
    "for model, predicted in prediction.items():\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n",
    "    cmp += 1\n",
    "\n",
    "\n",
    "plt.title('Classifiers comparaison with ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the accuracy, recall and f1-score for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['Logistic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues, labels=[\"positive\", \"negative\"]):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(y_test, prediction['Logistic'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix)    \n",
    "\n",
    "matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the accuracy, recall and f1-score for Naive Bayes Berrnoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['Bernoulli'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['Bernoulli'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues, labels=[\"positive\", \"negative\"]):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(y_test, prediction['Bernoulli'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix)    \n",
    "\n",
    "matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the accuracy, recall and f1-score for Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['Multinomial'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['Multinomial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues, labels=[\"positive\", \"negative\"]):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(y_test, prediction['Multinomial'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix)    \n",
    "\n",
    "matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the accuracy, recall and f1-score for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['SVM'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['SVM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues, labels=[\"positive\", \"negative\"]):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(y_test, prediction['SVM'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix)    \n",
    "\n",
    "matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the accuracy, recall and f1-score for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['KNN'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['KNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues, labels=[\"positive\", \"negative\"]):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(y_test, prediction['KNN'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix)    \n",
    "\n",
    "matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the accuracy, recall and f1-score for DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, prediction['DecisionTree'], target_names = [\"positive\", \"negative\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction['DecisionTree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, title='Confusion matrix', cmap=plt.cm.Blues, labels=[\"positive\", \"negative\"]):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "matrix = confusion_matrix(y_test, prediction['DecisionTree'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix)    \n",
    "\n",
    "matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(matrix_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying words depicting the best and worst features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = countVector.get_feature_names()\n",
    "feature_coefs = pd.DataFrame(\n",
    "    data = list(zip(features, logreg_result.coef_[0])),\n",
    "    columns = ['feature', 'coefficient'])\n",
    "\n",
    "feature_coefs.sort_values(by='coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the sentiments of few reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSentiments(model, testData):\n",
    "    testCounts = countVector.transform([testData])\n",
    "    testTfidf = tfidf_transformer.transform(testCounts)\n",
    "    result = model.predict(testTfidf)[0]\n",
    "    probability = model.predict_proba(testTfidf)[0]\n",
    "    print(\"Sample estimated as %s: negative prob %f, positive prob %f\" % (result.upper(), probability[0], probability[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_review=testSentiments(logreg, input('Enter Product review:'))\n",
    "testing_review=testSentiments(logreg, input('Enter Product review:'))\n",
    "testing_review=testSentiments(logreg, input('Enter Product review:'))\n",
    "print('****************************END*******************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
